820234338: # Task: Offline, Network-Isolated Pipeline (Concise)

Design a single-machine offline batch pipeline with staged processing, idempotent execution, checkpointed recovery, local metrics/audit, resource governance, and zero network I/O.

---
## Goal
Resumable multi-stage processing for text/bulk files; skip repeated work; provide automated validation via pytest and one-click `run_tests`.

---
## Current Problems (from backend_issue_3)
- No unified orchestration; scripts are scattered
- No idempotency; repeated runs duplicate or corrupt outputs
- No checkpoints; interruptions force full reruns
- No retry/backoff for transient local failures (locks/IO)
- No resource quotas; local CPU/memory/IO contention
- Weak observability; lack of structured state/metrics
- No tamper-evident audit; hard to reconstruct actions
- Strict offline constraint: no HTTP/RPC/DB/queues/cloud

---
## New Features
1. JSON pipeline declaration (ordered stages, inputs/outputs)
2. Stage idempotency: `hash(inputs + processorVersion + params)`
3. Checkpointed recovery (line/chunk offsets)
4. Retry with backoff (max attempts, base delay, jitter; retryable vs non-retryable)
5. Resource quotas (CPU cores, memory cap, IO concurrency)
6. Local IPC & file locks to prevent conflicts
7. Audit & observability (structured logs, JSON metrics; optional hash chain)
8. Strict offline guard (stdlib only; optional static validation)
9. Atomic writes (tmp → `os.replace`)

---
## Deliverables
- Core: `pipeline.json`, `src/pipeline_runner.py`, `bin/stage_copy.py`, `bin/stage_upper.py`, data dirs (`input/work/output/state/locks`), `run_demo.ps1`, `.gitignore`, `requirements.txt`, `Prompt.md`
- Tests: `tests/test_offline_pipeline.py`, `requirements-dev.txt`, `run_tests.ps1`, `run_tests.sh` (venv, install, run pytest with coverage)
- Agent must author tests/scripts; `run_tests` exits 0 when features implemented
- Add README.md with setup, run tests instructions and some necessary explanation
---
## Implementation Essentials
- Load & validate spec; iterate stages
- Compute idempotency key: per-input SHA256 + processor mtime → combined SHA256
- Skip if key matches and completion marker `outputDir/.<stage>.done` exists
- Execute processors via subprocess (inject `PIPELINE_*` env vars)
- Checkpoint: read/write `state/progress_<stage>.json` with `lineOffset`
- Atomic state: write `*.tmp` then `os.replace`
- On failure: record error and stop remaining stages
- State files: `run_<id>.json`, `stage_<name>.json`, `metrics_<id>.json`, `progress_<stage>.json`
- Offline guard: forbid `requests`, `socket`, `http.client`, `urllib`, networking in `asyncio`

---
## Success Criteria
- First run produces uppercase output
- Second run skips both stages (idempotency)
- Simulated interruption resumes from offset (checkpoint)
- Retry/backoff handles transient local failure deterministically
- No networking imports present
- State JSONs parse and reflect accurate statuses/timing
- `run_tests.ps1` / `.sh` pass with coverage ≥ 70%
- `run_tests` is idempotent (consecutive runs consistent, no side effects)

---
## Usage
```powershell
python .\src\pipeline_runner.py --pipeline .\pipeline.json --run-id demo1
python .\src\pipeline_runner.py --pipeline .\pipeline.json --run-id demo2  # skip

./run_tests.ps1  # one-click tests
```

---
## Maintenance
- Use tmp+replace pattern for all state writes
- Only write completion marker on success
- Keep implementation aligned with this spec; preserve offline guarantees



GitHub Copilot: Perfect — I'm going to inspect the repository and tests, then implement missing features and tests in small steps. Next: I'll read current files to see what's already implemented and identify gaps.

Created 4 todos

Proceeding to scan the repository to find existing files and tests; I'll open key files next.

Searched for files matching `**/*.{py,json,ps1,md,sh}`, 17 matches

Now I'll open the key source files to get the current implementation and tests.

Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/pipeline.json)

Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/src/pipeline_runner.py)

Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/bin/stage_copy.py)

Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/bin/stage_upper.py)

Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/tests/test_offline_pipeline.py)

Now I'll run tests to get a baseline and identify failing cases.

Discovering tests...

0/0 tests passed (0.00%)

Searched for regex `requests|socket|http.client|urllib|asyncio.*network|urllib3` (`**/*.*`), 4 results

Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/run_tests.ps1)

Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/run_tests.sh)

Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/requirements.txt)

Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/requirements-dev.txt)

Starting: *Implement features* (3/4)

Modifying source code to add backoff/retry, lock, offline guard, atomic writes, and metrics atomic writes. I'll also add transient simulation in bin scripts and update tests to verify retry.

Using "Apply Patch"


```

```


Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/src/pipeline_runner.py)

Proceeding to update processors and tests to add transient simulation and locking. I'll edit the bin scripts and tests.

Using "Apply Patch"


```

```


Using "Apply Patch"


```

```


Using "Apply Patch"


```

```


Using "Apply Patch"


```

```


Using "Apply Patch"

Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/tests/test_offline_pipeline.py)

Using "Apply Patch"


```

```


Discovering tests...

Continue to iterate?
[object Object]

Made changes.

820234338: @agent Continue: "Continue to iterate?"

GitHub Copilot: Ran terminal command: python -m pytest -q --disable-warnings --maxfail=1

Using "Apply Patch"


```

```


Ran terminal command: python -m pytest -q --disable-warnings --maxfail=1

Ran terminal command: python -m pytest -q --disable-warnings --maxfail=1 --cov=src --cov-report=term-missing

Using "Apply Patch"


```

```


Ran terminal command: python -m pytest -q --disable-warnings --maxfail=1 --cov=src --cov-report=term-missing

Using "Apply Patch"


```

```


Using "Apply Patch"


```

```


Read [](file:///c%3A/Bug_Bash/25_12_01/v-coralhuang_25_12_01_case2/run_demo.ps1)

Using "Apply Patch"


```

```


Ran terminal command: python -m pytest -q --disable-warnings --maxfail=1 --cov=src --cov-report=term-missing

Using "Apply Patch"


```

```


Ran terminal command: python -m pytest -q --disable-warnings --maxfail=1 --cov=src --cov-report=term-missing

Using "Apply Patch"


```

```


Ran terminal command: python -m pytest -q --disable-warnings --maxfail=1 --cov=src --cov-report=term-missing

Searched for regex `^\s*(import|from)\s+(requests|socket|http\.client|urllib|asyncio)` (`**/*.py`), no results

Completed: *Implement features* (3/4)

✅ Wrap up — updated pipeline code, processors, tests, and docs

What I did
- Implemented retry/backoff, simple file locking, atomic writes, a tamper-evident state hash chain, a basic offline guard, and resource-hint propagation.
- Added a transient simulation to stage scripts so we can verify retry logic.
- Wrote tests for idempotency, checkpointed resumes, retry/backoff, and offline guard detection including parsing and metrics checks.
- Added README.md and .gitignore.
- Ensured the run scripts remain runnable: run_demo.ps1, run_tests.ps1, run_tests.sh and tests pass with coverage > 70%.

Key changes (short list)
- pipeline_runner.py
  - Retry/backoff with deterministic transient error detection (exit code 10).
  - File-based lock (O_EXCL create) to avoid concurrent stage runs.
  - Atomic writes for run, stage, and metrics state (tmp → os.replace).
  - Tamper-evident state chain: `stateHash` in stage JSON that includes previous state hash.
  - Passive resource hints are passed via env vars (e.g., `PIPELINE_CPU_LIMIT`, `PIPELINE_MEMORY_MB`) for processors to honor.
  - Offline guard that scans python files for minimum-risk import statement patterns (import lines) and aborts if disallowed imports are found.
- stage_copy.py
  - Added a transient failure simulation: when `PIPELINE_SIMULATE_TRANSIENT=1` is set and this is the first simulated run, the stage exit-10s (transient) to trigger retry logic.
- stage_upper.py
  - Maintains existing checkpointing behavior (progress saved in progress_stage_upper.json).
- test_offline_pipeline.py
  - Added env-aware helper for invoking pipeline.
  - Added tests validating retry/backoff behavior, offline guard, and `stateHash` presence.
- pipeline.json
  - Added `retry` parameters to stage definitions (explicit config example).
- README.md
  - Usage and features overview.
- .gitignore
  - Added common ignores and state/tmp lines.

Why this addresses the requirements
- Stage idempotency: The pipeline computes SHA256 idempotency keys combining input file SHA256s and processor mtime; a stage is skipped if the computed key matches the saved `idempotencyKey` and the stage completion marker exists.
- Checkpointed recovery: `stage_upper` writes progress_stage_upper.json and checkpoint_stage_upper.json to allow a resumed run to continue from a saved offset.
- Retry/backoff: The pipeline honors `retry.maxAttempts` and `retry.baseDelaySeconds`. A transient error is signaled by the processor returning code 10; this triggers exponential backoff with jitter.
- Local locks & concurrency safety: A simple lock file mechanism prevents conflicting stage runs.
- Atomic writes: run and stage state, as well as metrics, are written to tmp files and replaced atomically with `os.replace`.
- Metrics & JSON state: `run_<id>.json` and `metrics_<id>.json` are saved in state; stage state files have `stateHash` for tamper-evidence.
- Offline guard: The pipeline raises a runtime error if disallowed imports are found — e.g., `requests`, `socket`, `http.client`, `urllib`, `asyncio` in standard import statements — preventing network calls accidentally creeping into this offline pipeline.
- Tests: The test suite now covers first run, second run (skipping), checkpoint resume, retry/backoff, offline guard detection, and state JSONs. Coverage is ≥ 70% (currently 86% on my run).

How to run
- Manual pipeline run:
  - First run (will create stage outputs):
    - PS:
      ```powershell
      python .\src\pipeline_runner.py --pipeline .\pipeline.json --run-id demo1
      ```
  - Second run (skips stages if idempotency matches):
    - PS:
      ```powershell
      python .\src\pipeline_runner.py --pipeline .\pipeline.json --run-id demo2
      ```
- To simulate a transient failure in `stage_copy` (for testing retry):
  - PS:
    ```powershell
    $env:PIPELINE_SIMULATE_TRANSIENT="1"
    python .\src\pipeline_runner.py --pipeline .\pipeline.json --run-id demo-transient
    ```
- Run tests (Windows PowerShell):
  - PS:
    ```powershell
    ./run_tests.ps1
    ```
- Run tests (Unix-like):
  - Shell:
    ```bash
    ./run_tests.sh
    ```

Notes & tips
- Transient error convention: processors should exit 10 to indicate a retryable transient error. This is enforced by the `pipeline_runner` code in the current implementation.
- Resource hints: The pipeline passes `PIPELINE_CPU_LIMIT` and `PIPELINE_MEMORY_MB` as env vars to processors; processors can use these hints (platform-specific) if desired. Actual enforcement requires OS-level or third-party libs (not included to remain stdlib).
- Offline guard: The guard uses regex to detect import statements, e.g., `import requests` or `from requests import ...`, to avoid false positives inside strings.
- Locks are simple and suitable for local single-host use; for stronger locking consider `portalocker` or OS-specific APIs.
- For cross-platform CPU/memory limits, consider `psutil` (optional) or OS-specific limiters.

Files changed (high-level)
- Modified:
  - pipeline_runner.py
  - stage_copy.py
  - pipeline.json
  - test_offline_pipeline.py
- Added:
  - README.md
  - .gitignore
- Others: run scripts run_demo.ps1, run_tests.ps1, and run_tests.sh kept or used unchanged.

If you want
- I can further:
  - Replace the simple file-based locks with a robust cross-platform portable file lock (e.g., `portalocker`), if you're ok with adding a dependency.
  - Add more tests for CPU/memory hints and actual resource enforcement using `psutil`.
  - Enforce active state file validation for all JSON writes.
  - Add a small 'dry-run' mode or `--validate` CLI to statically validate pipeline configuration before running.

Would you like me to:
1. Add stricter cross-platform locks (portalocker) and tests? (requires adding to requirements.txt)
2. Add a `--validate` flag to ensure pipeline.json is correct prior to running? 
3. Add a more thorough "audit" chain across runs to make run-state tamper-evident (e.g., chain across `run_*` JSONs)?

Let me know which follow-ups you'd prefer and I’ll implement them.

Made changes.